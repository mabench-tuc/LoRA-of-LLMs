{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mabench-tuc/LoRA-of-LLMs/blob/main/Gpt_2_FT_with_LoRA_on_E2E_NLG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P59i8nC2S4X5"
      },
      "source": [
        "##Setup Installation Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FZKZ8_SfW-H"
      },
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/microsoft/LoRA\n",
        "!pip install -qU bitsandbytes datasets accelerate loralib transformers peft trl\n",
        "!pip install datasets\n",
        "!pip install -U sacrebleu evaluate rouge-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NOS2DRx5Zmr"
      },
      "source": [
        "## Model Loading\n",
        "Here we load the model with its weights, the tokenizer and the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYlyO-Pf1MjZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModelForSeq2SeqLM, TrainingArguments\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRBU3szZ3s5o"
      },
      "source": [
        "### Load the GPT-2 Large model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_iuz13t3vGS"
      },
      "outputs": [],
      "source": [
        "# Move the model to the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the GPT-2 Large model and tokenizer\n",
        "print(\"Loading gpt2-large model...\")\n",
        "gpt2_large_model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\").to(device)\n",
        "\n",
        "gpt2_large_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
        "print(\"Successfully loaded gpt2-large model.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i1qJfcfKbuY"
      },
      "outputs": [],
      "source": [
        "model=gpt2_large_model\n",
        "tokenizer= gpt2_large_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZM9OU7ZNCfPR"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNyAqDqS60SP"
      },
      "source": [
        "## Post-processing on the model\n",
        "### Freezing the original weights\n",
        "we need to apply some post-processing on the n-bit model to enable training, let's freeze all our layers, and cast the layer-norm in floatm for stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf9F9Ax47s5s"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1L1j8hJDOXc"
      },
      "source": [
        "###Display Trainable Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbF93a9JDYSx"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    #Prints the number of trainable parameters in the model.\n",
        "\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC6y12GwZm9A"
      },
      "source": [
        "##Parameter Efficient Fine Tuning\n",
        "###Set up the LoRA Adapter\n",
        "Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using `get_peft_model` utility function from peft."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxqVR993Zyzo"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\"],\n",
        "    #target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "## target_modules='v', This represents the value projection layer in the transformer model. The value projection layer transforms input tokens into value vectors,\n",
        "# which are the actual values that are attended to based on the attention scores computed from query and key vectors.\n",
        "\n",
        "## target_modules='q',This typically refers to the query projection layer in a transformer-based model. The query projection layer is responsible for transforming\n",
        "# input tokens into query vectors, which are used to attend to other tokens in the sequence during self-attention mechanism.\n",
        "\n",
        "#c_attn: This is the convolution layer that computes the query, key, and value projections. The \"q_proj\" and \"v_proj\" are part of this layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hGvGLEkadq6"
      },
      "source": [
        "###Display trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtbcvxZdaopV"
      },
      "outputs": [],
      "source": [
        "model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uUxTfsEcLCN"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "We can simply load our dataset from ðŸ¤— Hugging Face with the `load_dataset` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nqJoaqPEFbPU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Text Generation dataset (E2E NLG Challenge)\n",
        "dataset = load_dataset(\"GEM/e2e_nlg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvuG--xXsr_F"
      },
      "source": [
        "###Tokenization of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQrx12fZ8G9K"
      },
      "outputs": [],
      "source": [
        "# Add padding token for GPT-2\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tokenize (dynamic padding instead of fixed 512)\n",
        "tokenized_datasets = dataset.map(\n",
        "    lambda x: tokenizer(x[\"meaning_representation\"], truncation=True, padding=\"longest\"),\n",
        "    batched=True\n",
        ")\n",
        "# Display an example of the tokenized dataset\n",
        "print(tokenized_datasets[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZu14GLXfU7v"
      },
      "outputs": [],
      "source": [
        "# GPT-2-specific settings: Add padding tokens, as GPT-2 does not use padding by default\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 uses <|endoftext|> as a padding token\n",
        "\n",
        "# Define the tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"meaning_representation\"],           # The \"data\" column contains the text in the E2E NLG dataset\n",
        "        max_length=512,             # Max sequence length for GPT-2\n",
        "        truncation=True,            # Truncate sequences longer than 512 tokens\n",
        "        padding=\"max_length\"        # Pad sequences shorter than 512 tokens\n",
        "    )\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Display an example of the tokenized dataset\n",
        "print(tokenized_datasets[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acTpMS4GZgIT"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeM1Yt88oPVK"
      },
      "source": [
        "We create a smaller subset of the full dataset to fine-tune our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3awRhnDONVT-"
      },
      "outputs": [],
      "source": [
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(15000))\n",
        "#\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(700))\n",
        "small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1400))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbwM5rP6bRNO"
      },
      "source": [
        "##Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kWBgLRil-tm"
      },
      "outputs": [],
      "source": [
        "#Import the necessary modules from the transformers library\n",
        "import transformers\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFjE4wHM0cOu"
      },
      "source": [
        "###Train LoRA Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVNqCyEzaGO0"
      },
      "outputs": [],
      "source": [
        "#LoRA paper for hyperparameters for GPT-2 Medium\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output_lora_gpt2\",  # Directory for saving the model\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=500,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs_lora_gpt2\",  # Directory for logging\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,  # Keep only 2 model checkpoints\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",  # Disable reporting to WandB or other loggers\n",
        "    fp16=True,  # Enable mixed precision training if you have a GPU\n",
        "    #bf16=True\n",
        "\n",
        ")\n",
        "\n",
        "# Define a custom data collator for causal language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False  # Causal LM does not use Masked Language Modeling (MLM)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7ULFlpagycC"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNa4zRIhgeex"
      },
      "outputs": [],
      "source": [
        "# Initialize SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    peft_config=lora_config,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9g0pwrR-SFC"
      },
      "source": [
        "### Pushing the Model to the Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u6tw3ALlqTn"
      },
      "outputs": [],
      "source": [
        "HUGGING_FACE_USER_NAME = \"\"\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYrIiPCSmABC"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt-2-Large-lora\"\n",
        "\n",
        "model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvg_E67bmEBg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"gpt-2-Large-lora\"\n",
        "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "config.base_model_name_or_path = \"gpt2-large\"\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "satE-Za85McS"
      },
      "outputs": [],
      "source": [
        "# Load the Lora model\n",
        "lora_model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "q5BTq3AfgG_m"
      },
      "outputs": [],
      "source": [
        "print(lora_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfOWTW0Q3oJG"
      },
      "source": [
        "## Memory Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crie5f8D4Xlk"
      },
      "outputs": [],
      "source": [
        "!pip install nvidia-ml-py3\n",
        "!pip install pynvml\n",
        "import pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL7QvqvSnq1e"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Yn0keImlEqt"
      },
      "outputs": [],
      "source": [
        "def print_gpu_memory():\n",
        "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "    print(f\"Cached memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
        "    print(f\"GPU utilization: {torch.cuda.utilization()}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFJXJMuBcCrJ"
      },
      "outputs": [],
      "source": [
        "print_gpu_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH7VyTPslK2W"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "print(\"\\nAfter emptying cache:\")\n",
        "print_gpu_memory()\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A3U9pVa4jnN"
      },
      "source": [
        "##Benchmark on E2E Nlg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liwSOKe-AMOE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVGEMLVs5Bi7"
      },
      "outputs": [],
      "source": [
        "# Load the LoRA model\n",
        "lora_model = PeftModel.from_pretrained(model, peft_model_id).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlpAmQCoBLdv"
      },
      "outputs": [],
      "source": [
        "# Load the E2E NLG dataset\n",
        "dataset = load_dataset(\"e2e_nlg\")  # Automatically downloads the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vakRPORoBCWM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# GPT-2-specific settings: Add padding tokens, as GPT-2 does not use padding by default\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 uses <|endoftext|> as a padding token\n",
        "\n",
        "# Step 3: Define the tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"meaning_representation\"],           # The \"data\" column contains the text in the E2E NLG dataset\n",
        "        max_length=512,             # Max sequence length for GPT-2\n",
        "        truncation=True,            # Truncate sequences longer than 512 tokens\n",
        "        padding=\"max_length\"        # Pad sequences shorter than 512 tokens\n",
        "    )\n",
        "\n",
        "# Step 4: Tokenize the dataset\n",
        "tokenized_e2e_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Step 5: Display an example of the tokenized dataset\n",
        "print(tokenized_e2e_dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOWmWX8uDxGj"
      },
      "outputs": [],
      "source": [
        "tokenized_e2e_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVkZj65BX3Da"
      },
      "outputs": [],
      "source": [
        "#small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_e2e_dataset[\"test\"].shuffle(seed=42).select(range(700))\n",
        "small_val_dataset = tokenized_e2e_dataset[\"validation\"].shuffle(seed=42).select(range(500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TL2p9Bn4tvO"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(example):\n",
        "    \"\"\"Concatenate the input and output for evaluation.\"\"\"\n",
        "    return {\n",
        "        \"input_text\": example[\"meaning_representation\"],\n",
        "        \"target_text\": example[\"human_reference\"],\n",
        "    }\n",
        "# Preprocess the dataset\n",
        "#processed_data= tokenized_e2e_dataset.map(preprocess_data)\n",
        "\n",
        "# Ensure processed_data is initialized as a dictionary\n",
        "processed_data = {}\n",
        "# Preprocess the validation and test datasets\n",
        "processed_data[\"validation\"] = small_val_dataset.map(preprocess_data)\n",
        "processed_data[\"test\"] = small_eval_dataset.map(preprocess_data)\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"meaning_representation\"],\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "# Evaluation function\n",
        "def evaluate_model(model, tokenizer, dataset):\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    bleu_scores = []\n",
        "    rouge_scores = []\n",
        "\n",
        "    # Initialize ROUGE scorer\n",
        "    rouge_scorer_instance = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "\n",
        "    for example in dataset:\n",
        "        input_text = example[\"input_text\"]\n",
        "        target_text = example[\"target_text\"]\n",
        "\n",
        "        # Tokenize and generate predictions\n",
        "        input_ids = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "        attention_mask = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\").attention_mask.to(model.device) # Generate attention mask\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=100, num_beams=5, early_stopping=True, pad_token_id=tokenizer.eos_token_id) # Pass attention_mask to generate\n",
        "\n",
        "        # Decode predictions\n",
        "        prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Compute BLEU\n",
        "        bleu_score = sentence_bleu(\n",
        "            [target_text.split()], prediction.split(), smoothing_function=smoothing\n",
        "        )\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "        # Compute ROUGE\n",
        "        rouge = rouge_scorer_instance.score(target_text, prediction)\n",
        "        rouge_scores.append({\n",
        "            \"rouge1\": rouge[\"rouge1\"].fmeasure,\n",
        "            \"rouge2\": rouge[\"rouge2\"].fmeasure,\n",
        "            \"rougeL\": rouge[\"rougeL\"].fmeasure,\n",
        "        })\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    avg_rouge = {\n",
        "        \"rouge1\": sum([r[\"rouge1\"] for r in rouge_scores]) / len(rouge_scores),\n",
        "        \"rouge2\": sum([r[\"rouge2\"] for r in rouge_scores]) / len(rouge_scores),\n",
        "        \"rougeL\": sum([r[\"rougeL\"] for r in rouge_scores]) / len(rouge_scores),\n",
        "    }\n",
        "\n",
        "    return avg_bleu, avg_rouge\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating the model...\")\n",
        "validation_data = processed_data[\"validation\"]\n",
        "avg_bleu, avg_rouge = evaluate_model(lora_model, tokenizer, validation_data)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"Average BLEU: {avg_bleu:.4f}\")\n",
        "print(f\"Average ROUGE-1: {avg_rouge['rouge1']:.4f}\")\n",
        "print(f\"Average ROUGE-2: {avg_rouge['rouge2']:.4f}\")\n",
        "print(f\"Average ROUGE-L: {avg_rouge['rougeL']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTcsrfuWgoPt"
      },
      "source": [
        "##Perform Inference\n",
        "\n",
        "###Preprocess the input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9k2BRNujS0P"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoqG0xbygvBk"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt = \"Once upon a time,\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytgex6Gqg8N2"
      },
      "source": [
        "###Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V8qF99poiJUP"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = lora_model(**inputs)\n",
        "\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m8yfnBygzDu"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "output_ids = lora_model.generate(\n",
        "    inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    max_length=50,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "output_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTDOeD4rhB1T"
      },
      "source": [
        "###Post-process the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYSbbXgvhDbC"
      },
      "outputs": [],
      "source": [
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcrSeS_vk3dc"
      },
      "source": [
        "### Inference of GPT-2 Large Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv7nV86glD3N"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "output_ids = gpt2_large_model.generate(\n",
        "    inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    max_length=50,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "output_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0v-P_jKlGzw"
      },
      "outputs": [],
      "source": [
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVqsg9KJZZJ8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vfOWTW0Q3oJG"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}