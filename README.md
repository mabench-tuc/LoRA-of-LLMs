# LoRA-of-LLMs
This project explores Low-Rank Adaptation (LoRA) for fine-tuning Large Language Models (LLMs) on NLP tasks. By updating only low-rank weight matrices, LoRA reduces trainable parameters and memory costs while maintaining strong performance. Experiments evaluate efficiencyâ€“effectiveness trade-offs.
