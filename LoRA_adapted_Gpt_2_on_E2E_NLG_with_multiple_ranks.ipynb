{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mabench-tuc/LoRA-of-LLMs/blob/main/LoRA_adapted_Gpt_2_on_E2E_NLG_with_multiple_ranks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P59i8nC2S4X5"
      },
      "source": [
        "##Setup Installation Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FZKZ8_SfW-H"
      },
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/microsoft/LoRA\n",
        "!pip install -qU bitsandbytes accelerate loralib transformers peft trl\n",
        "!pip install \"datasets<4.0.0\"\n",
        "! pip install -U sacrebleu evaluate rouge-score\n",
        "#!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYlyO-Pf1MjZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModelForSeq2SeqLM, TrainingArguments\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import bitsandbytes as bnb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpLj52ifMzl_"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NOS2DRx5Zmr"
      },
      "source": [
        "## Model's Loading\n",
        "Here we load the model with its weights and the tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRBU3szZ3s5o"
      },
      "source": [
        "## Load the GPT-2 Large model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_iuz13t3vGS"
      },
      "outputs": [],
      "source": [
        "# Move the model to the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the GPT-2 Large model and tokenizer\n",
        "print(\"Loading gpt2-large model...\")\n",
        "gpt2_large_model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\").to(device)\n",
        "\n",
        "gpt2_large_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
        "print(\"Successfully loaded gpt2-large model.\")\n",
        "\n",
        "OUTPUT_DIR = \"./lora_e2e_mranks_results\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i1qJfcfKbuY"
      },
      "outputs": [],
      "source": [
        "model=gpt2_large_model\n",
        "tokenizer= gpt2_large_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evsmo3pf2pz1"
      },
      "outputs": [],
      "source": [
        "print(model)\n",
        "#print_trainable_parameters(gpt2_large_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNyAqDqS60SP"
      },
      "source": [
        "## Post-processing on the model\n",
        "### Freezing the original weights\n",
        "We need to apply some post-processing on the n-bit model to enable training, let's freeze all our layers, and cast the layer-norm in floatm for stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf9F9Ax47s5s"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1L1j8hJDOXc"
      },
      "source": [
        "###Display Trainable Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbF93a9JDYSx"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    #Prints the number of trainable parameters in the model.\n",
        "\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtbcvxZdaopV"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxDA4YdHeyzy"
      },
      "source": [
        "## Load E2E NLG Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nqJoaqPEFbPU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"GEM/e2e_nlg\")\n",
        "\n",
        "# Add padding token for GPT-2\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tokenize (dynamic padding instead of fixed 512)\n",
        "tokenized_datasets = dataset.map(\n",
        "    lambda x: tokenizer(x[\"meaning_representation\"], truncation=True, padding=\"longest\"),\n",
        "    batched=True\n",
        ")\n",
        "# Display an example of the tokenized dataset\n",
        "print(tokenized_datasets[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unnpxZi8e9tw"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvuG--xXsr_F"
      },
      "source": [
        "###Tokenization of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gZu14GLXfU7v"
      },
      "outputs": [],
      "source": [
        "# GPT-2-specific settings: Add padding tokens, as GPT-2 does not use padding by default\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 uses <|endoftext|> as a padding token\n",
        "\n",
        "# Step 3: Define the tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"meaning_representation\"],           # The \"data\" column contains the text in the E2E NLG dataset\n",
        "        max_length=512,             # Max sequence length for GPT-2\n",
        "        truncation=True,            # Truncate sequences longer than 512 tokens\n",
        "        padding=\"max_length\"        # Pad sequences shorter than 512 tokens\n",
        "    )\n",
        "\n",
        "# Step 4: Tokenize the dataset\n",
        "tokenized_e2e_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Step 5: Display an example of the tokenized dataset\n",
        "print(tokenized_e2e_dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acTpMS4GZgIT"
      },
      "outputs": [],
      "source": [
        "#tokenized_dataset= tokenized_e2e_dataset\n",
        "tokenized_dataset= tokenized_datasets\n",
        "#tokenized_e2e_dataset.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5pW1b9XxeSt"
      },
      "outputs": [],
      "source": [
        "# See available splits\n",
        "print(dataset)\n",
        "\n",
        "# Print the column headers for the training split\n",
        "print(\"Column headers:\", dataset[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeM1Yt88oPVK"
      },
      "source": [
        "We create a smaller subset of the full dataset to fine-tune our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY6G3xqHvNsb"
      },
      "outputs": [],
      "source": [
        "train_data= tokenized_dataset[\"train\"].shuffle(seed=42).select(range(10000))\n",
        "val_data= tokenized_dataset[\"validation\"].shuffle(seed=42).select(range(1000))\n",
        "eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbwM5rP6bRNO"
      },
      "source": [
        "##Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kWBgLRil-tm"
      },
      "outputs": [],
      "source": [
        "#Import the necessary modules from the transformers library\n",
        "import transformers\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from trl import SFTTrainer\n",
        "import evaluate\n",
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFjE4wHM0cOu"
      },
      "source": [
        "###Train LoRA Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YspNE80I9FGs"
      },
      "outputs": [],
      "source": [
        "# filepath: experiments/lora_rank_sweep_eval.py\n",
        "\n",
        "\"\"\"\n",
        "Run a LoRA rank sweep, train each model, evaluate with BLEU + ROUGE, and save CSV:\n",
        "Columns: rank, # of trainable parameters, bleu, rouge1, rouge2, rougeL\n",
        "Assumptions:\n",
        " - `train_data`, `val_data`, `tokenized_dataset[\"validation\"]`, and `tokenizer` exist in the environment.\n",
        " - required packages: torch, transformers, datasets, peft, accelerate (optional), nltk, rouge_score, pandas\n",
        "\"\"\"\n",
        "\n",
        "from typing import Tuple, Dict, List\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import AutoModelForCausalLM, TrainingArguments\n",
        "# If using SFTTrainer from trl or a custom package, adapt import accordingly:\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# ---- USER VARIABLES / PREPARATION ----\n",
        "# Ensure these variables are present in your environment:\n",
        "# tokenizer, train_data, val_data, tokenized_dataset\n",
        "# If not present, load/create them before running this script.\n",
        "\n",
        "# This script expects the datasets to be huggingface datasets or lists of dicts with keys:\n",
        "# For evaluation dataset: each item must have \"input_text\" and \"target_text\"\n",
        "\n",
        "\n",
        "def preprocess_data(example):\n",
        "    \"\"\"Maps existing dataset columns to 'input_text' and 'target_text'.\"\"\"\n",
        "    # Assuming the tokenized dataset has 'meaning_representation' and 'target' columns\n",
        "    return {\n",
        "        \"input_text\": example[\"meaning_representation\"],\n",
        "        \"target_text\": example[\"target\"], # Assuming 'target' is the correct column name for human reference\n",
        "    }\n",
        "\n",
        "# Create processed_data from the tokenized dataset splits\n",
        "processed_data = {}\n",
        "# Assuming 'tokenized_dataset' is a DatasetDict with 'validation' and 'test' splits\n",
        "processed_data[\"validation\"] = tokenized_dataset[\"validation\"].map(preprocess_data)\n",
        "processed_data[\"test\"] = tokenized_dataset[\"test\"].map(preprocess_data)\n",
        "\n",
        "\n",
        "# ---- EVALUATION FUNCTION ----\n",
        "def evaluate_model(model, tokenizer, dataset, device: str = None, max_gen_len: int = 100) -> Tuple[float, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Given a causal LM (model) and tokenizer, iterate dataset of dicts with keys:\n",
        "    'input_text' and 'target_text', generate predictions, compute average BLEU and ROUGE scores.\n",
        "    Returns: (avg_bleu, avg_rouge_dict)\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    bleu_scores: List[float] = []\n",
        "    rouge_scores_list: List[Dict[str, float]] = []\n",
        "\n",
        "    rouge_scorer_instance = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "\n",
        "    # If dataset is a Dataset object, iterate with .select or plain iteration works\n",
        "    for example in tqdm(dataset, desc=\"Evaluating\", leave=False):\n",
        "        input_text = example[\"input_text\"]\n",
        "        target_text = example[\"target_text\"]\n",
        "\n",
        "        # Tokenize input for generation\n",
        "        enc = tokenizer(\n",
        "            input_text,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512\n",
        "        )\n",
        "        input_ids = enc.input_ids.to(device)\n",
        "        attention_mask = enc.attention_mask.to(device) if hasattr(enc, \"attention_mask\") else None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gen_kwargs = {\"max_new_tokens\": max_gen_len, \"num_beams\": 5, \"early_stopping\": True, \"pad_token_id\": tokenizer.eos_token_id}\n",
        "            if attention_mask is not None:\n",
        "                output_ids = model.generate(input_ids, attention_mask=attention_mask, **gen_kwargs)\n",
        "            else:\n",
        "                output_ids = model.generate(input_ids, **gen_kwargs)\n",
        "\n",
        "        prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
        "        ref = target_text.strip()\n",
        "\n",
        "        # BLEU (sentence-level)\n",
        "        try:\n",
        "            bleu_score = sentence_bleu([ref.split()], prediction.split(), smoothing_function=smoothing)\n",
        "        except Exception:\n",
        "            bleu_score = 0.0\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "        # ROUGE\n",
        "        rouge = rouge_scorer_instance.score(ref, prediction)\n",
        "        rouge_scores_list.append({\n",
        "            \"rouge1\": rouge[\"rouge1\"].fmeasure,\n",
        "            \"rouge2\": rouge[\"rouge2\"].fmeasure,\n",
        "            \"rougeL\": rouge[\"rougeL\"].fmeasure,\n",
        "        })\n",
        "\n",
        "    # Avoid division by zero\n",
        "    n = max(1, len(bleu_scores))\n",
        "    avg_bleu = sum(bleu_scores) / n\n",
        "    avg_rouge = {\n",
        "        \"rouge1\": sum(r[\"rouge1\"] for r in rouge_scores_list) / max(1, len(rouge_scores_list)),\n",
        "        \"rouge2\": sum(r[\"rouge2\"] for r in rouge_scores_list) / max(1, len(rouge_scores_list)),\n",
        "        \"rougeL\": sum(r[\"rougeL\"] for r in rouge_scores_list) / max(1, len(rouge_scores_list)),\n",
        "    }\n",
        "\n",
        "    return avg_bleu, avg_rouge\n",
        "\n",
        "\n",
        "# ---- SWEEP & TRAIN ----\n",
        "def run_lora_rank_sweep(\n",
        "    ranks,\n",
        "    tokenizer,\n",
        "    train_data,\n",
        "    val_data,\n",
        "    processed_validation_dataset,\n",
        "    output_csv: str = \"lora_rank_results.csv\",\n",
        "    reduce_val_size: int = 500\n",
        "):\n",
        "    results = []\n",
        "\n",
        "    for r in ranks:\n",
        "        print(f\"\\n===== Training with LoRA rank {r} =====\\n\")\n",
        "\n",
        "        config = LoraConfig(\n",
        "            r=r,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"attn.c_attn\"],\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "\n",
        "        # Load fresh base model for each rank\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "        base_model.config.pad_token_id = base_model.config.eos_token_id\n",
        "\n",
        "        lora_model = get_peft_model(base_model, config)\n",
        "\n",
        "        # Training args — adjust as needed\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f\"./results/rank_{r}\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            learning_rate=2e-4,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            warmup_steps=500,\n",
        "            num_train_epochs=5,\n",
        "            per_device_train_batch_size=1,\n",
        "            per_device_eval_batch_size=1,\n",
        "            gradient_accumulation_steps=4,\n",
        "            gradient_checkpointing=True,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            dataloader_pin_memory=True,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir=\"./logs_lorarank_gpt2\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=20,\n",
        "            save_total_limit=2,\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            optim=\"adamw_torch\",\n",
        "        )\n",
        "\n",
        "        # Prepare train/val tokenized datasets for SFTTrainer expected format:\n",
        "        # create labels column for causal LM training (copy of input_ids)\n",
        "        train_with_labels = train_data.add_column(\"labels\", train_data[\"input_ids\"])\n",
        "        val_small = val_data.select(range(min(reduce_val_size, len(val_data))))\n",
        "        val_with_labels = val_small.add_column(\"labels\", val_small[\"input_ids\"])\n",
        "\n",
        "        # If SFTTrainer is available, use it. If not, adapt to trainer class.\n",
        "        try:\n",
        "            trainer = SFTTrainer(\n",
        "                model=lora_model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_with_labels,\n",
        "                eval_dataset=val_with_labels,\n",
        "                compute_metrics=None,  # we'll compute BLEU/ROUGE separately below\n",
        "            )\n",
        "\n",
        "            trainer.train()\n",
        "            # trainer.evaluate() can be left or used; we will compute BLEU/ROUGE using our evaluator for consistency.\n",
        "            _ = trainer.evaluate()\n",
        "        except NameError:\n",
        "            # If SFTTrainer is not available, skip training (or integrate your own trainer here)\n",
        "            print(\"SFTTrainer not found in this environment. Skipping trainer.train().\")\n",
        "        except Exception as e:\n",
        "            print(f\"Trainer failed with exception: {e}. Attempting to continue to evaluation.\")\n",
        "\n",
        "        # Count trainable parameters\n",
        "        trainable_count = int(sum(p.numel() for p in lora_model.parameters() if p.requires_grad))\n",
        "\n",
        "        # Evaluate on processed_validation_dataset (we'll use a reduced subset for speed)\n",
        "        # If processed_validation_dataset is huge, select a smaller slice:\n",
        "        try:\n",
        "            val_for_eval = processed_validation_dataset.select(range(min(500, len(processed_validation_dataset))))\n",
        "        except Exception:\n",
        "            # If .select isn't available, assume it's an iterable and slice\n",
        "            val_for_eval = list(processed_validation_dataset)[:500]\n",
        "\n",
        "        avg_bleu, avg_rouge = evaluate_model(lora_model, tokenizer, val_for_eval)\n",
        "\n",
        "        results.append({\n",
        "            \"rank\": r,\n",
        "            \"# of trainable parameters\": trainable_count,\n",
        "            \"bleu\": avg_bleu,\n",
        "            \"rouge1\": avg_rouge[\"rouge1\"],\n",
        "            \"rouge2\": avg_rouge[\"rouge2\"],\n",
        "            \"rougeL\": avg_rouge[\"rougeL\"],\n",
        "        })\n",
        "\n",
        "        # Cleanup\n",
        "        del lora_model, base_model\n",
        "        if 'trainer' in locals():\n",
        "            try:\n",
        "                del trainer\n",
        "            except Exception:\n",
        "                pass\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Save results to CSV\n",
        "    df = pd.DataFrame(results, columns=[\"rank\", \"# of trainable parameters\", \"bleu\", \"rouge1\", \"rouge2\", \"rougeL\"])\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Saved LoRA rank sweep results to {output_csv}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---- MAIN EXECUTION ----\n",
        "if __name__ == \"__main__\":\n",
        "    ranks = [1, 4, 8, 16, 32]\n",
        "    # Ensure the following variables are defined in your session:\n",
        "    # tokenizer, train_data, val_data, processed_data[\"validation\"]\n",
        "    # If they are not, load them here.\n",
        "\n",
        "    # Ensure processed_data is created before calling run_lora_rank_sweep\n",
        "    # This block should be executed before the sweep function call\n",
        "    def preprocess_data_main(example):\n",
        "        \"\"\"Maps existing dataset columns to 'input_text' and 'target_text' for the main block.\"\"\"\n",
        "        # Assuming the tokenized dataset has 'meaning_representation' and 'target' columns\n",
        "        return {\n",
        "            \"input_text\": example[\"meaning_representation\"],\n",
        "            \"target_text\": example[\"target\"], # Assuming 'target' is the correct column name\n",
        "        }\n",
        "\n",
        "    # Create processed_data from the tokenized dataset splits\n",
        "    # Assuming 'tokenized_dataset' is a DatasetDict with 'validation' and 'test' splits\n",
        "    processed_data = {}\n",
        "    processed_data[\"validation\"] = tokenized_dataset[\"validation\"].map(preprocess_data_main)\n",
        "    processed_data[\"test\"] = tokenized_dataset[\"test\"].map(preprocess_data_main)\n",
        "\n",
        "\n",
        "    df_results = run_lora_rank_sweep(\n",
        "        ranks=ranks,\n",
        "        tokenizer=tokenizer,\n",
        "        train_data=train_data,\n",
        "        val_data=val_data,\n",
        "        processed_validation_dataset=processed_data[\"validation\"], # Use the correctly processed dataset\n",
        "        output_csv=\"lora_rank_results.csv\",\n",
        "        reduce_val_size=500,\n",
        "    )\n",
        "\n",
        "    print(df_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OnRJewN9Qc2"
      },
      "source": [
        "##GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsEAen67NKug"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxYCttojRMDe"
      },
      "source": [
        "## Visualizations of LoRA Adaptation of GPT-2 on E2E NLG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLXYO2ksVAjB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load results file (adjust path if needed)\n",
        "df = pd.read_csv(\"lora_rank_results.csv\")\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "# Plot all metrics\n",
        "plt.plot(df['rank'], df['bleu'], marker='o', color='C0', label='BLEU')\n",
        "plt.plot(df['rank'], df['rouge1'], marker='s', color='C1', label='ROUGE-1')\n",
        "plt.plot(df['rank'], df['rouge2'], marker='^', color='C2', label='ROUGE-2')\n",
        "plt.plot(df['rank'], df['rougeL'], marker='v', color='C3', label='ROUGE-L')\n",
        "\n",
        "# Annotate each point with six decimal values\n",
        "for i, r in enumerate(df['rank']):\n",
        "    plt.text(r, df['bleu'][i], f\"{df['bleu'][i]:.6f}\", ha='center', va='bottom', fontsize=8, color='C0')\n",
        "    plt.text(r, df['rouge1'][i], f\"{df['rouge1'][i]:.6f}\", ha='center', va='bottom', fontsize=8, color='C1')\n",
        "    plt.text(r, df['rouge2'][i], f\"{df['rouge2'][i]:.3f}\", ha='center', va='bottom', fontsize=8, color='C2')\n",
        "    plt.text(r, df['rougeL'][i], f\"{df['rougeL'][i]:.6f}\", ha='center', va='bottom', fontsize=8, color='C3')\n",
        "\n",
        "# Labels and formatting\n",
        "plt.xlabel(\"LoRA Rank\", fontsize=12)\n",
        "plt.ylabel(\"Score\", fontsize=12)\n",
        "plt.title(\"BLEU and ROUGE Metrics vs LoRA Rank\", fontsize=14)\n",
        "plt.xticks(df['rank'])\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"bleu_rouge_vs_lora_ranks.png\", dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jobz9mO4W68J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "# Plot each metric\n",
        "plt.plot(df['# of trainable parameters'], df['bleu'], marker='o', label='BLEU', color='C0')\n",
        "plt.plot(df['# of trainable parameters'], df['rouge1'], marker='s', label='ROUGE-1', color='C1')\n",
        "plt.plot(df['# of trainable parameters'], df['rouge2'], marker='^', label='ROUGE-2', color='C2')\n",
        "plt.plot(df['# of trainable parameters'], df['rougeL'], marker='v', label='ROUGE-L', color='C3')\n",
        "\n",
        "# Annotate each point with six decimal places\n",
        "for i, row in df.iterrows():\n",
        "    plt.text(row['# of trainable parameters'], row['bleu'], f\"{row['bleu']:.6f}\", fontsize=8, color='C0', ha='left', va='bottom')\n",
        "    plt.text(row['# of trainable parameters'], row['rouge1'], f\"{row['rouge1']:.6f}\", fontsize=8, color='C1', ha='left', va='bottom')\n",
        "    plt.text(row['# of trainable parameters'], row['rouge2'], f\"{row['rouge2']:.3f}\", fontsize=8, color='C2', ha='left', va='bottom')\n",
        "    plt.text(row['# of trainable parameters'], row['rougeL'], f\"{row['rougeL']:.6f}\", fontsize=8, color='C3', ha='left', va='bottom')\n",
        "\n",
        "# Formatting\n",
        "plt.xlabel(\"Number of Trainable Parameters\", fontsize=12)\n",
        "plt.ylabel(\"Score\", fontsize=12)\n",
        "plt.title(\"BLEU and ROUGE Metrics vs Trainable Parameters\", fontsize=14)\n",
        "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save figure\n",
        "plt.savefig(\"combined_bleu_rouge_vs_params.png\", dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS-RL36haHrC"
      },
      "outputs": [],
      "source": [
        "# Metric efficiency vs LoRA rank: BLEU/ROUGE per million params + composite efficiency\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams.update({\"font.size\": 12})\n",
        "\n",
        "fn = \"lora_rank_results.csv\"\n",
        "df = pd.read_csv(fn)\n",
        "\n",
        "# --- required columns check ---\n",
        "req = [\"rank\", \"# of trainable parameters\", \"bleu\", \"rouge1\", \"rouge2\", \"rougeL\"]\n",
        "missing = [c for c in req if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns in {fn}: {missing}. Rename them or update the CSV.\")\n",
        "\n",
        "# --- prepare quantities ---\n",
        "df = df.copy()\n",
        "df[\"params_million\"] = df[\"# of trainable parameters\"].astype(float) / 1e6\n",
        "# avoid division by zero\n",
        "if (df[\"params_million\"] <= 0).any():\n",
        "    raise ValueError(\"Found non-positive values in '# of trainable parameters'\")\n",
        "\n",
        "# efficiencies: score per million parameters\n",
        "df[\"eff_bleu\"]   = df[\"bleu\"].astype(float)   / df[\"params_million\"]\n",
        "df[\"eff_rouge1\"] = df[\"rouge1\"].astype(float) / df[\"params_million\"]\n",
        "df[\"eff_rouge2\"] = df[\"rouge2\"].astype(float) / df[\"params_million\"]\n",
        "df[\"eff_rougeL\"] = df[\"rougeL\"].astype(float) / df[\"params_million\"]\n",
        "\n",
        "# composite efficiency: normalize each metric by its max across ranks (range 0..1), sum, then divide by params_million\n",
        "eps = 1e-12\n",
        "norm_bleu   = df[\"bleu\"].astype(float)   / (df[\"bleu\"].astype(float).max()   + eps)\n",
        "norm_r1     = df[\"rouge1\"].astype(float) / (df[\"rouge1\"].astype(float).max() + eps)\n",
        "norm_r2     = df[\"rouge2\"].astype(float) / (df[\"rouge2\"].astype(float).max() + eps)\n",
        "norm_rL     = df[\"rougeL\"].astype(float) / (df[\"rougeL\"].astype(float).max() + eps)\n",
        "df[\"composite_norm\"] = norm_bleu + norm_r1 + norm_r2 + norm_rL   # range ~0..4\n",
        "df[\"eff_composite\"] = df[\"composite_norm\"] / df[\"params_million\"]\n",
        "\n",
        "# sort by rank for plotting\n",
        "df.sort_values(by=\"rank\", inplace=True)\n",
        "ranks = df[\"rank\"].values\n",
        "\n",
        "# --- plotting ---\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "\n",
        "# plot individual efficiencies\n",
        "ax.plot(ranks, df[\"eff_bleu\"],   marker='o', linestyle='-', label='BLEU per M params',   zorder=4)\n",
        "ax.plot(ranks, df[\"eff_rouge1\"], marker='s', linestyle='-', label='ROUGE-1 per M params', zorder=4)\n",
        "ax.plot(ranks, df[\"eff_rouge2\"], marker='^', linestyle='-', label='ROUGE-2 per M params', zorder=4)\n",
        "ax.plot(ranks, df[\"eff_rougeL\"], marker='v', linestyle='-', label='ROUGE-L per M params', zorder=4)\n",
        "\n",
        "# plot composite efficiency on secondary axis (keeps scale readable)\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(ranks, df[\"eff_composite\"], marker='D', linestyle='--', color='black', label='Composite efficiency', zorder=5)\n",
        "\n",
        "# annotate each point with six decimal places (main metrics)\n",
        "for i, r in enumerate(ranks):\n",
        "    x = r\n",
        "    # annotate the composite on the right axis with 6 decimals\n",
        "    ax2.text(x, df[\"eff_composite\"].iloc[i], f\"{df['eff_composite'].iloc[i]:.6f}\",\n",
        "             fontsize=9, ha='left', va='bottom', color='black')\n",
        "    # small annotations for BLEU (example) — avoid overcrowding by offsetting slightly\n",
        "    ax.text(x, df[\"eff_bleu\"].iloc[i], f\"{df['eff_bleu'].iloc[i]:.6f}\",\n",
        "            fontsize=8, ha='right', va='bottom', color=ax.get_lines()[0].get_color())\n",
        "\n",
        "# axis labels, title, ticks\n",
        "ax.set_xlabel(\"LoRA rank\", fontsize=13)\n",
        "ax.set_ylabel(\"Score per million parameters (BLEU / ROUGE)\", fontsize=13)\n",
        "ax2.set_ylabel(\"Composite normalized score per million parameters\", fontsize=13)\n",
        "ax.set_title(\"Parameter-efficiency of LoRA ranks: per-metric and composite measures\", fontsize=14)\n",
        "\n",
        "ax.set_xticks(ranks)\n",
        "ax.grid(True, linestyle='--', alpha=0.4)\n",
        "\n",
        "# legends: combine handles from both axes\n",
        "h1, l1 = ax.get_legend_handles_labels()\n",
        "h2, l2 = ax2.get_legend_handles_labels()\n",
        "ax.legend(h1 + h2, l1 + l2, loc='upper right', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"efficiency_vs_rank.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# --- brief printed summary for thesis reporting ---\n",
        "print(\"Rank | params (M) | eff_bleu | eff_rougeL | eff_composite\")\n",
        "for _, row in df.iterrows():\n",
        "    print(f\"{int(row['rank']):>4} | {row['params_million']:.3f} M | \"\n",
        "          f\"{row['eff_bleu']:.6f} | {row['eff_rougeL']:.6f} | {row['eff_composite']:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsukJKKQmHEK"
      },
      "source": [
        "##Inference for NLG Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tjMPss2CkVOi"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizergpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# Load the tokenizer and model\n",
        "\n",
        "modelgpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR_RUsoLZSqM"
      },
      "outputs": [],
      "source": [
        "tokenizer=tokenizergpt2\n",
        "model= modelgpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AteOUw2lJHQ"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt = \"Once upon a time\"\n",
        "#prompt= \"Write a Python function to check if a number is prime.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF5_DrRDly4i"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "output_ids = model.generate(\n",
        "    inputs.input_ids.to(model.device),\n",
        "    attention_mask=inputs.attention_mask.to(model.device),\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    max_length=50,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "output_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTs7Zp4xl2Bj"
      },
      "outputs": [],
      "source": [
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}