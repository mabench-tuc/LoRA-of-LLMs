{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mabench-tuc/LoRA-of-LLMs/blob/main/LoRA_Gpt_2_NL_to_Code_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etBjV9UdfvS0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CBKWaRkfVU6"
      },
      "outputs": [],
      "source": [
        "import nbformat, os, shutil\n",
        "FOLDER = \"/content/My Drive/ModSem/Github\"\n",
        "\n",
        "for fname in os.listdir(FOLDER):\n",
        "    if fname.endswith(\".ipynb\"):\n",
        "        path = os.path.join(FOLDER, fname)\n",
        "        backup = path + \".backup.ipynb\"\n",
        "        shutil.copy2(path, backup)\n",
        "        nb = nbformat.read(path, as_version=4)\n",
        "        changed = False\n",
        "        for cell in nb.cells:\n",
        "            if \"widgets\" in cell.get(\"metadata\", {}):\n",
        "                cell[\"metadata\"].pop(\"widgets\", None)\n",
        "                changed = True\n",
        "            if cell.get(\"outputs\"):\n",
        "                cell[\"outputs\"] = []\n",
        "                changed = True\n",
        "            if cell.get(\"execution_count\") is not None:\n",
        "                cell[\"execution_count\"] = None\n",
        "                changed = True\n",
        "        if changed:\n",
        "            nbformat.write(nb, path)\n",
        "            print(\"Cleaned:\", path)\n",
        "        else:\n",
        "            print(\"OK:\", path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P59i8nC2S4X5"
      },
      "source": [
        "##Setup Installation Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FZKZ8_SfW-H"
      },
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/microsoft/LoRA\n",
        "!pip install -qU bitsandbytes datasets accelerate loralib transformers peft trl\n",
        "#!pip install -U datasets\n",
        "!pip install -U sacrebleu evaluate rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYlyO-Pf1MjZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModelForSeq2SeqLM, TrainingArguments\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import bitsandbytes as bnb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NOS2DRx5Zmr"
      },
      "source": [
        "## Model's loading\n",
        "Here we load the model with its weights and the tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRBU3szZ3s5o"
      },
      "source": [
        "## Load the GPT-2 Large model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_iuz13t3vGS"
      },
      "outputs": [],
      "source": [
        "# Move the model to the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the GPT-2 Large model and tokenizer\n",
        "print(\"Loading gpt2-large model...\")\n",
        "gpt2_large_model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\").to(device)\n",
        "\n",
        "gpt2_large_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
        "print(\"Successfully loaded gpt2-large model.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i1qJfcfKbuY"
      },
      "outputs": [],
      "source": [
        "model=gpt2_large_model\n",
        "tokenizer= gpt2_large_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNyAqDqS60SP"
      },
      "source": [
        "## Post-processing on the model\n",
        "### Freezing the original weights\n",
        "Finally, we need to apply some post-processing on the n-bit model to enable training, let's freeze all our layers, and cast the layer-norm in floatm for stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf9F9Ax47s5s"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1L1j8hJDOXc"
      },
      "source": [
        "###Display Trainable Parameters\n",
        "\n",
        "This is a function to print out how much LoRA reduces the number of trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbF93a9JDYSx"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    #Prints the number of trainable parameters in the model.\n",
        "\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZeFqTYOqJF3"
      },
      "source": [
        "#### Model Architecture\n",
        "\n",
        "It's important to observe the model's construction so you can ensure you know which modules you should apply LoRA to.\n",
        "\n",
        "As per the paper, we're going to focus on the attention weights - so keep an eye out for modules like: `q_proj`, `v_proj`, `query_key_value`. This is model dependent - In our case (GPT-2), the target module is `attn.c_attn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evsmo3pf2pz1"
      },
      "outputs": [],
      "source": [
        "print(model)\n",
        "#print_trainable_parameters(gpt2_large_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC6y12GwZm9A"
      },
      "source": [
        "##Parameter Efficient Fine Tuning\n",
        "###Set up the LoRA Adapter\n",
        "Here comes the magic with `peft`! Let's load a `PeftModel` and specify that we are going to use low-rank adapters (LoRA) using `get_peft_model` utility function from `peft`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxqVR993Zyzo"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"attn.c_attn\"],\n",
        "    #target_modules=[\"query_key_value\"],\n",
        "    #target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "## target_modules='v', This represents the value projection layer in the transformer model. The value projection layer transforms input tokens into value vectors,\n",
        "# which are the actual values that are attended to based on the attention scores computed from query and key vectors.\n",
        "\n",
        "## target_modules='q',This typically refers to the query projection layer in a transformer-based model. The query projection layer is responsible for transforming\n",
        "# input tokens into query vectors, which are used to attend to other tokens in the sequence during self-attention mechanism.\n",
        "\n",
        "#c_attn: This is the convolution layer that computes the query, key, and value projections. The \"q_proj\" and \"v_proj\" are part of this layer.\n",
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hGvGLEkadq6"
      },
      "source": [
        "###Display trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtbcvxZdaopV"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pu1VHElR6x9"
      },
      "source": [
        "## Merge the CodeAlpaca and MBPP Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pY2QQ1G6it1"
      },
      "outputs": [],
      "source": [
        "# File: merge_mbpp_codealpaca_dataset.py\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "import random\n",
        "\n",
        "# Load datasets\n",
        "mbpp = load_dataset(\"mbpp\")\n",
        "# Load CodeAlpaca dataset using the correct identifier\n",
        "codealpaca = load_dataset(\"sahil2801/CodeAlpaca-20k\")\n",
        "\n",
        "merged_data = []\n",
        "\n",
        "# Format MBPP: as instruction-tuning format with test cases included\n",
        "for row in mbpp[\"train\"]:\n",
        "    # Construct the prompt in the desired format\n",
        "    prompt = f\"### Instruction\\n{row['text'].strip()}\\n\\n### Input\\n{row['code'].strip()}\\n\\n### Test Cases\\n{row['test_list']}\"\n",
        "    merged_data.append({\n",
        "        \"instruction\": row[\"text\"].strip(),\n",
        "        \"input\": row[\"code\"].strip(),\n",
        "        \"output\": row[\"code\"].strip(), # Using code as output for MBPP based on context\n",
        "        \"source\": \"mbpp\",\n",
        "        \"text\": prompt # Add the formatted prompt to the data\n",
        "    })\n",
        "\n",
        "# Format CodeAlpaca\n",
        "for row in codealpaca[\"train\"]:\n",
        "    # Construct the prompt in the desired format\n",
        "    prompt = f\"### Instruction\\n{row['instruction'].strip()}\\n\\n### Input\\n{row['input'].strip()}\\n\\n### Response:\\n{row['output'].strip()}\"\n",
        "    merged_data.append({\n",
        "        \"instruction\": row[\"instruction\"].strip(),\n",
        "        \"input\": row[\"input\"].strip(),\n",
        "        \"output\": row[\"output\"].strip(),\n",
        "        \"source\": \"codealpaca\",\n",
        "        \"text\": prompt # Add the formatted prompt to the data\n",
        "    })\n",
        "\n",
        "\n",
        "# Shuffle merged dataset\n",
        "random.shuffle(merged_data)\n",
        "\n",
        "# Save as HuggingFace Dataset\n",
        "merged_dataset = Dataset.from_list(merged_data)\n",
        "merged_dataset.save_to_disk(\"merged_mbpp_codealpaca\")\n",
        "\n",
        "print(\"✅ Merged dataset saved to: merged_mbpp_codealpaca\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Yu2AEJ3-rjp"
      },
      "outputs": [],
      "source": [
        "# prompt: tokenize the merged_dataset\n",
        "\n",
        "# Set the padding token for the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = merged_dataset.map(tokenize_function, batched=True)\n",
        "print(\"✅ Merged dataset tokenized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1j4xWIc02H6"
      },
      "outputs": [],
      "source": [
        "dataset = merged_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLfHQzyO_LW1"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets=tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOiJKPUC_rSJ"
      },
      "outputs": [],
      "source": [
        "# prompt: split the tokenized_datasets into \"train\", \"test\" and \"vlidation\"\n",
        "\n",
        "# Define split ratios\n",
        "train_ratio = 0.8\n",
        "test_ratio = 0.1\n",
        "validation_ratio = 0.1\n",
        "\n",
        "# Split the dataset\n",
        "train_test_validation_split = tokenized_datasets.train_test_split(test_size=test_ratio + validation_ratio, seed=42)\n",
        "test_validation_split = train_test_validation_split[\"test\"].train_test_split(test_size=validation_ratio / (test_ratio + validation_ratio), seed=42)\n",
        "\n",
        "train_dataset = train_test_validation_split[\"train\"]\n",
        "test_dataset = test_validation_split[\"train\"]\n",
        "validation_dataset = test_validation_split[\"test\"]\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(validation_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uUxTfsEcLCN"
      },
      "source": [
        "## Load CodeAlpaca Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGsIZVVfcFOP"
      },
      "outputs": [],
      "source": [
        "### Code Dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"sahil2801/CodeAlpaca-20k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFut5KSW8Vo-"
      },
      "source": [
        "### Dataset Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "t6PxAKlR7EsZ"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Combine into one dataset and shuffle\n",
        "ds_all = dataset['train'].shuffle(seed=42)\n",
        "\n",
        "# Split into train (80%), val (10%), test (10%)\n",
        "ds_split = ds_all.train_test_split(test_size=0.2, seed=42)\n",
        "ds_temp = ds_split['test'].train_test_split(test_size=0.5, seed=42)\n",
        "ds_dict = DatasetDict({\n",
        "    'train': ds_split['train'],\n",
        "    'validation': ds_temp['train'],\n",
        "    'test': ds_temp['test']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "648zSlyP4dgG"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token  # ensure padding token is set\n",
        "# Define prompt template\n",
        "def format_prompt(example):\n",
        "    prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "# Apply prompt formatting\n",
        "ds_dict = ds_dict.map(format_prompt)\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    tokens = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "# Tokenize the dataset\n",
        "ds_tokenized = ds_dict.map(tokenize_function, batched=True, remove_columns=ds_dict[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HGrP0aF57HG"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets=ds_tokenized\n",
        "tokenized_datasets.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Gmsc7a-4LG_T"
      },
      "outputs": [],
      "source": [
        "#Extract a small portion of the\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(3000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(700))\n",
        "small_val_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(700))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbwM5rP6bRNO"
      },
      "source": [
        "##Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kWBgLRil-tm"
      },
      "outputs": [],
      "source": [
        "#Import the necessary modules from the transformers library\n",
        "import transformers\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFjE4wHM0cOu"
      },
      "source": [
        "###Train LoRA Adapter\n",
        "\n",
        "The `Trainer` class contains all the usual hyper-parameters from traditional ML applications!\n",
        "\n",
        "If you're running into CUDA memory issues - please modify both the `per_device_train_batch_size` to be lower, and also reduce `r` in your LoRAConfig."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVNqCyEzaGO0"
      },
      "outputs": [],
      "source": [
        "#LoRA paper for hyperparameters for GPT-2 Medium\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output_lora_gpt2\",  # Directory for saving the model\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=500,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs_lora_gpt2\",  # Directory for logging\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,  # Keep only 2 model checkpoints\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",  # Disable reporting to WandB or other loggers\n",
        "    fp16=True,  # Enable mixed precision training if you have a GPU\n",
        ")\n",
        "\n",
        "# Define a custom data collator for causal language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False  # Causal LM does not use Masked Language Modeling (MLM)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9cTYmqKp-9a"
      },
      "outputs": [],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    #train_dataset=tokenized_datasets[\"train\"],\n",
        "    #eval_dataset=tokenized_datasets.get(\"validation\", None),  # Use validation if available\n",
        "    #tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tWSYrQ34DXF4"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9g0pwrR-SFC"
      },
      "source": [
        "### Pushing the Model to the Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u6tw3ALlqTn"
      },
      "outputs": [],
      "source": [
        "HUGGING_FACE_USER_NAME = \"mabc-3\"\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYrIiPCSmABC"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt-2-LLoRA-NLP2Code\"\n",
        "\n",
        "model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybA4WlidT1HQ"
      },
      "source": [
        "### Load Adapters from the Hub\n",
        "\n",
        "You can also directly load adapters from the Huggingface Hub using the commands below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk8ckPeVada2"
      },
      "outputs": [],
      "source": [
        "HUGGING_FACE_USER_NAME = \"mabc-3\"\n",
        "model_name = \"gpt-2-LLoRA-NLP2Code\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvg_E67bmEBg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "satE-Za85McS"
      },
      "outputs": [],
      "source": [
        "# Load the Lora model\n",
        "lora_model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "27hVGHHGUVoe"
      },
      "outputs": [],
      "source": [
        "print(lora_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OnRJewN9Qc2"
      },
      "source": [
        "##GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsEAen67NKug"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXesld71cQUF"
      },
      "source": [
        "## Inference on Code Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5x8w2yUFWHC"
      },
      "source": [
        "###LoRA Model vs Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrXMkq16Vgw6"
      },
      "outputs": [],
      "source": [
        "base_model_name = \"gpt2-large\" ##\"gpt2\"  # Or use gpt2-large\n",
        "\n",
        "# Load tokenizer\n",
        "gpt2_large_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "# ✅ STEP 4: Load Full Model (no LoRA)\n",
        "def load_full_model():\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "480VloFSFWeL"
      },
      "outputs": [],
      "source": [
        "# ✅ Shared settings\n",
        "#prompt = \"Write a Python function to check if a number is prime\"\n",
        "#prompt =\"Find the product of all digits in a string, ignoring non-digit characters.\"\n",
        "#prompt=\"Return a list of all substrings of length 3 that are palindromes in the given string.\"\n",
        "prompt= \"write a function in python to check if a number is a power of two without using loops or recursion.\"\n",
        "#prompt= \"Sort a list of tuples by the second element in descending order, then by the first element ascending if tie. data = [(1, 3), (2, 3), (3, 2), (4, 4)\"\n",
        "#prompt= \"Write a Python function that takes a list of integers and returns the list sorted in descending order.\"\n",
        "#prompt= \"Write a Python function that returns the factorial of a number using recursion.\"\n",
        "\n",
        "max_new_tokens = 250\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "# ✅ STEP 6: Benchmarking Function\n",
        "def benchmark_model(model, tokenizer, label=\"Model\"):\n",
        "    # Ensure padding token is set for the tokenizer\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
        "\n",
        "    # Warm-up\n",
        "    _ = model.generate(**inputs, max_new_tokens=10)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    end = time.time()\n",
        "\n",
        "    latency = end - start\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\n📌 {label} Result:\")\n",
        "    print(\"Generated Output:\\n\", decoded)\n",
        "    print(f\"⏱ Latency: {latency:.3f} seconds\")\n",
        "    print(f\"💾 Peak GPU Memory: {peak_mem:.2f} MB\")\n",
        "    return latency, peak_mem\n",
        "\n",
        "\n",
        "# ✅ STEP 7: Run Comparison\n",
        "full_model = load_full_model() #change this to BM\n",
        "lora_model = model ##change this to gpt-2 lora\n",
        "\n",
        "print(\"🧪 Benchmarking FULL Model...\")\n",
        "latency_full, mem_full = benchmark_model(full_model, gpt2_large_tokenizer, label=\"Full GPT-2\")\n",
        "\n",
        "print(\"\\n🧪 Benchmarking LoRA Model...\")\n",
        "latency_lora, mem_lora = benchmark_model(lora_model, tokenizer, label=\"LoRA GPT-2\")\n",
        "\n",
        "# ✅ STEP 8: Summary Comparison\n",
        "print(\"\\n📊 COMPARISON SUMMARY\")\n",
        "print(f\"Latency: Full = {latency_full:.3f}s | LoRA = {latency_lora:.3f}s\")\n",
        "print(f\"Memory:  Full = {mem_full:.2f} MB | LoRA = {mem_lora:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cd1Ielh_b19"
      },
      "source": [
        "### Python Programming Prompts\n",
        "\n",
        "Practical coding prompts designed to illustrate and assess the fundamental programming capabilities of the fine-tuned LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_WsZcafejBE"
      },
      "outputs": [],
      "source": [
        "def factorial(n):\n",
        "    if n == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return n * factorial(n-1)\n",
        "\n",
        "print(factorial(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Pz_JRO-eFNJ"
      },
      "outputs": [],
      "source": [
        "factorial(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86meKNYrcRVM"
      },
      "outputs": [],
      "source": [
        "def sort_list(numbers):\n",
        "    return sorted(numbers)\n",
        "\n",
        "numbers= [13, 2, 33, 43, 15]\n",
        "print(sort_list(numbers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvbKgBAvg7M0"
      },
      "outputs": [],
      "source": [
        "def is_power_of_two_without_loops(num):\n",
        "    if num % 2 == 0:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzwpoZzqdHTI"
      },
      "outputs": [],
      "source": [
        "def is_power_of_two(num):\n",
        "    if num % 2 == 0:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPMWG3-Adb--"
      },
      "outputs": [],
      "source": [
        "print(is_power_of_two_without_loops(16))\n",
        "print(is_power_of_two_without_loops(18))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOVdMJpnOPoL"
      },
      "source": [
        "##Evaluation of the Model\n",
        "\n",
        "###Evaluation on HumanEval Benchmark\n",
        "\n",
        "This evaluation framework tests the fine-tuned GPT-2 model on HumanEval by generating code completions, executing them safely, and measuring correctness through Pass@k scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEvzXyMvha6N"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6dyeb2gWd3Sa"
      },
      "outputs": [],
      "source": [
        "# File: evaluate_lora_gpt2_humaneval.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from datasets import load_dataset\n",
        "from accelerate import init_empty_weights, infer_auto_device_map\n",
        "from peft import PeftModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Config\n",
        "#MODEL_PATH = \"path/to/your/lora/fine-tuned/gpt2\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAX_NEW_TOKENS = 170   #128\n",
        "NUM_SAMPLES = 5\n",
        "GENERATION_ARGS = dict(do_sample=True, temperature=0.8, max_new_tokens=MAX_NEW_TOKENS, num_return_sequences=NUM_SAMPLES)\n",
        "\n",
        "# Load tokenizer and model\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)\n",
        "#model = GPT2LMHeadModel.from_pretrained(MODEL_PATH, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        "model = lora_model\n",
        "model.eval()\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Load HumanEval dataset\n",
        "dataset = load_dataset(\"openai_humaneval\")\n",
        "\n",
        "def evaluate_sample_k(prompt: str) -> list:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, **GENERATION_ARGS)\n",
        "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return [out[len(prompt):] for out in decoded_outputs]\n",
        "\n",
        "def main():\n",
        "    passed = 0\n",
        "    total = 0\n",
        "\n",
        "    print(\"\\n==================== Evaluation Start ====================\\n\")\n",
        "\n",
        "    for idx, item in enumerate(tqdm(dataset[\"test\"], desc=\"Evaluating\")):\n",
        "        prompt = item[\"prompt\"]\n",
        "        entry_point = item[\"entry_point\"]\n",
        "        canonical_solution = item[\"canonical_solution\"]\n",
        "\n",
        "        completions = evaluate_sample_k(prompt)\n",
        "        test_passed = False\n",
        "\n",
        "        for code in completions:\n",
        "            try:\n",
        "                local_env = {}\n",
        "                exec(prompt + code, local_env)\n",
        "                generated_func = local_env[entry_point]\n",
        "\n",
        "                reference_env = {}\n",
        "                exec(prompt + canonical_solution, reference_env)\n",
        "                reference_func = reference_env[entry_point]\n",
        "\n",
        "                match_all = True\n",
        "                for inp in item[\"test\"]:\n",
        "                    if eval(inp, {}, {entry_point: generated_func}) != eval(inp, {}, {entry_point: reference_func}):\n",
        "                        match_all = False\n",
        "                        break\n",
        "\n",
        "                if match_all:\n",
        "                    test_passed = True\n",
        "                    break\n",
        "\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        status = \"✅ PASS\" if test_passed else \"❌ FAIL\"\n",
        "        print(f\"[{idx+1:03}] {entry_point:<30} | {status}\")\n",
        "        print(\"→ Generated: \", completions[0].strip().splitlines()[0][:120])\n",
        "        print(\"→ Reference: \", canonical_solution.strip().splitlines()[0][:120])\n",
        "        print(\"------------------------------------------------------------\")\n",
        "\n",
        "        passed += int(test_passed)\n",
        "        total += 1\n",
        "\n",
        "    print(\"\\n==================== Evaluation Summary ====================\")\n",
        "    print(f\"Total Passed: {passed} / {total}\")\n",
        "    if total > 0:\n",
        "        print(f\"Pass@{NUM_SAMPLES} Score: {passed / total:.2%}\")\n",
        "    else:\n",
        "        print(\"Pass@k Score: N/A (No samples evaluated)\")\n",
        "    print(\"===========================================================\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81bdP9Utqjul"
      },
      "source": [
        "### Analysis:\n",
        "LoRA fine-tuned GPT-2 Large on CodeAlpaca performs well on code-style prompts but fails on the HumanEval dataset.\n",
        "\n",
        "*   CodeAlpaca focuses on instruction-style completions, mostly docstring-to-code or natural language to code.\n",
        "*   These prompts are simplified and often lack complex logic, recursion, or rigorous test cases.\n",
        "\n",
        "*   LoRA fine-tuning adapts your model well to generate syntactically-correct, readable code, especially if prompts resemble your training distribution.\n",
        "\n",
        "###Failure on HumanEval:\n",
        "HumanEval is a strict unit-test-driven benchmark. It requires:\n",
        "\n",
        "*   Full correctness (not just plausible code)\n",
        "*   Handling edge cases\n",
        "*   Understanding nuanced logic (e.g., dynamic programming, recursion, math, string parsing)\n",
        "\n",
        "▶ It fails to learn **deep functional reasoning**, which requires **diverse and structured training signals** — missing from CodeAlpaca."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRbez1ZUZiOO"
      },
      "source": [
        "##Switching between adapters\n",
        "You can “merge” two LoRA adapters, you apply both updates to the base model:\n",
        "\n",
        "\n",
        "You can also reload GPT-2 and swap adapters anytime:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWilAj8GZh-9"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, AutoModelForCausalLM\n",
        "\n",
        "# Load base GPT-2 again\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2 Large\")\n",
        "\n",
        "# Load CodeAlpaca LoRA\n",
        "codealpaca_model = PeftModel.from_pretrained(base_model, \"./lora-codealpaca\")\n",
        "\n",
        "# Load MBPP LoRA\n",
        "mbpp_model = PeftModel.from_pretrained(base_model, \"./lora-mbpp\")\n",
        "\n",
        "# Switch between them\n",
        "mbpp_model.set_adapter(\"mbpp_adapter\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKBuyCsT_XBu"
      },
      "source": [
        "## Further Fine-Tuning on MBPP Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8EPbT5FA-fW"
      },
      "outputs": [],
      "source": [
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMbT8HOK_PyA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1. Load MBPP dataset\n",
        "dataset = load_dataset(\"mbpp\")\n",
        "\n",
        "# 2. Choose tokenizer (GPT-2 in your case)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 has no pad token\n",
        "\n",
        "# 3. Preprocessing: join prompt + code solution\n",
        "def preprocess(batch):\n",
        "    # You can customize the prompt style here\n",
        "    text = [\n",
        "        f\"Problem: {p}\\nSolution:\\n{s}\"\n",
        "        for p, s in zip(batch[\"text\"], batch[\"code\"])\n",
        "    ]\n",
        "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "# 4. Apply preprocessing\n",
        "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "print(tokenized_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOy_3NNvBxK_"
      },
      "outputs": [],
      "source": [
        "# Add labels = input_ids to every split\n",
        "tokenized_dataset = tokenized_dataset.map(\n",
        "    lambda batch: {\"labels\": batch[\"input_ids\"]},\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "# Now you can safely pass train/val datasets\n",
        "train_dataset = tokenized_dataset[\"train\"]\n",
        "eval_dataset  = tokenized_dataset[\"validation\"]\n",
        "test_dataset = tokenized_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci5E6NGS1-oU"
      },
      "outputs": [],
      "source": [
        "print(tokenized_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK7H_cskA2mJ"
      },
      "outputs": [],
      "source": [
        "# metrics.py\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import Levenshtein\n",
        "import contextlib\n",
        "import io\n",
        "\n",
        "# Load Hugging Face metrics once\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def safe_exec(code, func_name, test_inputs, expected_outputs):\n",
        "    \"\"\"Run generated code in restricted env and validate against tests.\"\"\"\n",
        "    env = {}\n",
        "    try:\n",
        "        with contextlib.redirect_stdout(io.StringIO()):\n",
        "            exec(code, env)\n",
        "        func = env.get(func_name, None)\n",
        "        if func is None:\n",
        "            return False, 0.0\n",
        "\n",
        "        passed, total = 0, len(test_inputs)\n",
        "        for inp, exp in zip(test_inputs, expected_outputs):\n",
        "            out = func(*inp) if isinstance(inp, tuple) else func(inp)\n",
        "            if out == exp:\n",
        "                passed += 1\n",
        "        return passed == total, passed / total\n",
        "    except Exception:\n",
        "        return False, 0.0\n",
        "\n",
        "\n",
        "def pass_at_k(num_correct, num_samples, k):\n",
        "    \"\"\"Compute Pass@k as in HumanEval.\"\"\"\n",
        "    if num_samples == 0 or num_correct == 0:\n",
        "        return 0.0\n",
        "    return 1.0 - np.prod([\n",
        "        (1.0 - (num_correct / (num_samples - i)))\n",
        "        for i in range(min(k, num_samples - num_correct + 1))\n",
        "    ])\n",
        "\n",
        "\n",
        "def build_compute_metrics(tokenizer, k=3, mbpp_tests=None):\n",
        "    \"\"\"\n",
        "    Returns a compute_metrics function compatible with Hugging Face Trainer.\n",
        "    Captures tokenizer + optional MBPP test set.\n",
        "    \"\"\"\n",
        "    def compute_metrics(eval_preds):\n",
        "        preds, labels = eval_preds\n",
        "\n",
        "        # ---- Prepare predictions and labels ----\n",
        "        # If logits, take argmax over vocab\n",
        "        if preds.ndim == 3:  # (batch, seq_len, vocab_size)\n",
        "            preds = np.argmax(preds, axis=-1)\n",
        "\n",
        "        preds = preds.astype(int)\n",
        "        labels = labels.astype(int)\n",
        "\n",
        "        # Replace ignore index (-100) with pad token id\n",
        "        labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
        "\n",
        "        # Decode into strings\n",
        "        decoded_preds = [tokenizer.decode(p, skip_special_tokens=True) for p in preds]\n",
        "        decoded_labels = [tokenizer.decode(l, skip_special_tokens=True) for l in labels]\n",
        "\n",
        "        # ---- Text-based metrics ----\n",
        "        results = {\n",
        "            \"bleu\": bleu.compute(\n",
        "                predictions=decoded_preds, references=[[l] for l in decoded_labels]\n",
        "            )[\"bleu\"],\n",
        "            \"rougeL\": rouge.compute(\n",
        "                predictions=decoded_preds, references=decoded_labels\n",
        "            )[\"rougeL\"],\n",
        "            \"exact_match\": float(np.mean([\n",
        "                p.strip() == l.strip() for p, l in zip(decoded_preds, decoded_labels)\n",
        "            ])),\n",
        "            \"avg_edit_distance\": float(np.mean([\n",
        "                Levenshtein.distance(p, l) for p, l in zip(decoded_preds, decoded_labels)\n",
        "            ])),\n",
        "            \"pass@1\": None,\n",
        "            \"pass@k\": None,\n",
        "            \"exec_accuracy\": None,\n",
        "        }\n",
        "\n",
        "\n",
        "        # --- inside compute_metrics, MBPP block ---\n",
        "        if mbpp_tests is not None:\n",
        "        # Collect indices actually evaluated in this batch\n",
        "            eval_indices = [idx for idx in range(len(decoded_preds)) if idx in mbpp_tests]\n",
        "            num_correct = 0\n",
        "            exec_accs = []\n",
        "\n",
        "            for idx in eval_indices:\n",
        "              pred = decoded_preds[idx]\n",
        "              tests = mbpp_tests[idx]\n",
        "              func_name, inputs, outputs = tests[\"func_name\"], tests[\"inputs\"], tests[\"outputs\"]\n",
        "              passed_all, acc = safe_exec(pred, func_name, inputs, outputs)\n",
        "              if passed_all:\n",
        "                num_correct += 1\n",
        "              exec_accs.append(acc)\n",
        "\n",
        "            if exec_accs:\n",
        "              num_evaluated = len(exec_accs)\n",
        "              results[\"exec_accuracy\"] = float(np.mean(exec_accs))\n",
        "             # normalize by the number actually evaluated (not the full batch size)\n",
        "              results[\"pass@1\"] = num_correct / num_evaluated\n",
        "              results[\"pass@k\"] = pass_at_k(num_correct, num_evaluated, k)\n",
        "\n",
        "        return results\n",
        "\n",
        "    return compute_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky7qYzfrZnOw"
      },
      "outputs": [],
      "source": [
        "# timing_callback.py\n",
        "import time\n",
        "import csv\n",
        "import os\n",
        "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "\n",
        "class TimingCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    Records step times and epoch times. Writes a CSV `throughput_log.csv` with columns:\n",
        "    step, epoch, step_time_s, iter_time_s (same), samples_per_step, tokens_per_step, cumulative_time_s\n",
        "    \"\"\"\n",
        "    def __init__(self, out_path=\"throughput_log.csv\", seq_length=None):\n",
        "        self.start_step_time = None\n",
        "        self.cumulative_time = 0.0\n",
        "        self.out_path = out_path\n",
        "        self.seq_length = seq_length\n",
        "        # create header\n",
        "        if not os.path.exists(self.out_path):\n",
        "            with open(self.out_path, \"w\", newline=\"\") as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\"global_step\",\"epoch\",\"step_time_s\",\"samples_per_step\",\"tokens_per_step\",\"cumulative_time_s\"])\n",
        "\n",
        "    def on_step_begin(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        # called at start of each optimizer step (approx)\n",
        "        self.start_step_time = time.time()\n",
        "\n",
        "    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        # called at end of step\n",
        "        if self.start_step_time is None:\n",
        "            return\n",
        "        step_time = time.time() - self.start_step_time\n",
        "        self.cumulative_time += step_time\n",
        "\n",
        "        # derive samples_per_step:\n",
        "        # Trainer stores per_device_train_batch_size and gradient_accumulation_steps in args\n",
        "        per_device_bs = args.per_device_train_batch_size\n",
        "        accum = getattr(args, \"gradient_accumulation_steps\", 1)\n",
        "        # If using multiple devices, state.num_processes is total workers (world_size)\n",
        "        world_size = getattr(state, \"num_processes\", 1)\n",
        "        samples_per_step = per_device_bs * accum * world_size\n",
        "\n",
        "        if self.seq_length is None:\n",
        "            # fallback - if seq_length unknown, you can set it when constructing the callback\n",
        "            seq_length = getattr(args, \"max_seq_length\", None) or getattr(args, \"max_length\", None) or 512\n",
        "        else:\n",
        "            seq_length = self.seq_length\n",
        "\n",
        "        tokens_per_step = samples_per_step * seq_length\n",
        "\n",
        "        with open(self.out_path, \"a\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([state.global_step, getattr(state, \"epoch\", None), round(step_time,6), samples_per_step, tokens_per_step, round(self.cumulative_time,6)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU6fiVhuHg4q"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "mbpp_lora_config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"attn.c_attn\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfDtzf79HgCe"
      },
      "outputs": [],
      "source": [
        "# Add MBPP adapter without removing CodeAlpaca adapter\n",
        "lora_model.add_adapter(\"mbpp_adapter\", mbpp_lora_config)\n",
        "\n",
        "# Switch active adapter to MBPP (so training only updates this one)\n",
        "#lora_model.set_adapter(\"mbpp_adapter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N9TybnkEw21"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sft-mbpp-lora\",       # where to save checkpoints\n",
        "    num_train_epochs=5,                 # MBPP is small -> don’t need too many\n",
        "    per_device_train_batch_size=2,      # keep small to fit on free Colab GPUs\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,      # effective batch size = 16\n",
        "    learning_rate=2e-5,                 # safe LR for LoRA fine-tuning\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.05,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    fp16=False,\n",
        "    gradient_checkpointing=False,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        ")\n",
        "# Define a custom data collator for causal language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False  # Causal LM does not use Masked Language Modeling (MLM)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qrd2dC3OAEDr"
      },
      "outputs": [],
      "source": [
        "# optional: pass seq_length if you know it (e.g., 512)\n",
        "timing_cb = TimingCallback(out_path=\"throughput_log.csv\", seq_length=512)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[timing_cb], # Wrap the callback in a list\n",
        "    compute_metrics=build_compute_metrics(tokenizer, k=7, mbpp_tests=eval_dataset)\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save only the MBPP LoRA adapter\n",
        "model.save_pretrained(\"./lora-mbpp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ab5yK9AHcnJ"
      },
      "outputs": [],
      "source": [
        "model = lora_model\n",
        "# Save only the MBPP LoRA adapter\n",
        "model.save_pretrained(\"./lora-mbpp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFelnbajbt6b"
      },
      "outputs": [],
      "source": [
        "# After training\n",
        "logs = trainer.state.log_history\n",
        "\n",
        "# Collect evaluation history only\n",
        "eval_history = [entry for entry in logs if \"eval_loss\" in entry]\n",
        "\n",
        "# Convert into lists of steps and metrics\n",
        "steps = [entry[\"step\"] for entry in eval_history]\n",
        "\n",
        "h_metrics = {\n",
        "    \"bleu\": [entry.get(\"eval_bleu\") for entry in eval_history],\n",
        "    \"rougeL\": [entry.get(\"eval_rougeL\") for entry in eval_history],\n",
        "    \"exact_match\": [entry.get(\"eval_exact_match\") for entry in eval_history],\n",
        "    \"avg_edit_distance\": [entry.get(\"eval_avg_edit_distance\") for entry in eval_history],\n",
        "    \"pass@1\": [entry.get(\"eval_pass@1\") for entry in eval_history],\n",
        "    \"pass@k\": [entry.get(\"eval_pass@k\") for entry in eval_history],\n",
        "    \"exec_accuracy\": [entry.get(\"eval_exec_accuracy\") for entry in eval_history],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-6Heb6XGAJ6"
      },
      "outputs": [],
      "source": [
        "h_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WWkm5EtbtCU"
      },
      "source": [
        "##Visualizing Throughput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Umjtppbzee"
      },
      "outputs": [],
      "source": [
        "# Academic-style plotting for throughput_log.csv (from TimingCallback)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# === User parameters ===\n",
        "CSV_PATH = \"throughput_log.csv\"         # path you passed to TimingCallback\n",
        "OUT_DIR = \"./figures\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "WARMUP_STEPS = 50       # number of initial optimizer steps to exclude as warm-up (tune as needed)\n",
        "ROLL_WINDOW = 200       # rolling window (in steps) for smoothing; adjust to your log length\n",
        "SAVE_DPI = 300\n",
        "\n",
        "# === Read CSV ===\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Ensure expected numeric columns\n",
        "for c in [\"step_time_s\", \"samples_per_step\", \"tokens_per_step\", \"global_step\"]:\n",
        "    if c not in df.columns and c == \"global_step\":\n",
        "        # some callbacks wrote \"global_step\", else use index\n",
        "        df[\"global_step\"] = df.index\n",
        "        break\n",
        "\n",
        "# convert types\n",
        "df[\"step_time_s\"] = pd.to_numeric(df[\"step_time_s\"], errors=\"coerce\")\n",
        "df[\"samples_per_step\"] = pd.to_numeric(df[\"samples_per_step\"], errors=\"coerce\")\n",
        "df[\"tokens_per_step\"] = pd.to_numeric(df[\"tokens_per_step\"], errors=\"coerce\")\n",
        "\n",
        "# compute throughput metrics\n",
        "df[\"samples_per_sec\"] = df[\"samples_per_step\"] / df[\"step_time_s\"]\n",
        "df[\"tokens_per_sec\"] = df[\"tokens_per_step\"] / df[\"step_time_s\"]\n",
        "\n",
        "# rolling stats\n",
        "roll = lambda s: s.rolling(window=ROLL_WINDOW, min_periods=1, center=True).mean()\n",
        "df[\"tokens_per_sec_roll\"] = roll(df[\"tokens_per_sec\"])\n",
        "df[\"samples_per_sec_roll\"] = roll(df[\"samples_per_sec\"])\n",
        "df[\"tokens_per_sec_std\"] = df[\"tokens_per_sec\"].rolling(window=ROLL_WINDOW, min_periods=1).std()\n",
        "\n",
        "# steady-state selection (after warm-up)\n",
        "steady = df.iloc[WARMUP_STEPS : ].copy()\n",
        "if steady.empty:\n",
        "    steady = df.copy()\n",
        "\n",
        "# summary statistics (steady-state)\n",
        "summary = {\n",
        "    \"tokens_per_sec_mean\": steady[\"tokens_per_sec\"].mean(),\n",
        "    \"tokens_per_sec_median\": steady[\"tokens_per_sec\"].median(),\n",
        "    \"tokens_per_sec_p5\": steady[\"tokens_per_sec\"].quantile(0.05),\n",
        "    \"tokens_per_sec_p95\": steady[\"tokens_per_sec\"].quantile(0.95),\n",
        "    \"samples_per_sec_mean\": steady[\"samples_per_sec\"].mean(),\n",
        "    \"samples_per_sec_median\": steady[\"samples_per_sec\"].median()\n",
        "}\n",
        "print(\"Steady-state throughput summary (after warm-up):\")\n",
        "for k,v in summary.items():\n",
        "    print(f\"  {k}: {v:.2f}\")\n",
        "\n",
        "# === Plot settings for academic style ===\n",
        "plt.rcParams.update({\n",
        "    \"figure.figsize\": (9,4.5),\n",
        "    \"font.size\": 12,\n",
        "    \"axes.titlesize\": 13,\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"legend.fontsize\": 11,\n",
        "    \"xtick.labelsize\": 10,\n",
        "    \"ytick.labelsize\": 10,\n",
        "    \"savefig.dpi\": SAVE_DPI,\n",
        "    \"lines.linewidth\": 1.2\n",
        "})\n",
        "\n",
        "# ---- Figure 1: Tokens/sec over steps (with rolling avg + variability) ----\n",
        "fig, ax = plt.subplots()\n",
        "x = df[\"global_step\"] if \"global_step\" in df.columns else df.index\n",
        "ax.plot(x, df[\"tokens_per_sec\"], alpha=0.25, label=\"tokens/sec (per step)\")\n",
        "ax.plot(x, df[\"tokens_per_sec_roll\"], label=f\"tokens/sec (rolling, window={ROLL_WINDOW})\")\n",
        "# shaded ±1 std of rolling window (if available)\n",
        "std = df[\"tokens_per_sec_std\"].fillna(0)\n",
        "ax.fill_between(x, df[\"tokens_per_sec_roll\"] - std, df[\"tokens_per_sec_roll\"] + std, alpha=0.12)\n",
        "# annotate warm-up region\n",
        "if WARMUP_STEPS > 0:\n",
        "    warmup_x = x.iloc[:WARMUP_STEPS]\n",
        "    ax.axvspan(warmup_x.min(), warmup_x.max(), color='grey', alpha=0.08, label=\"warm-up (excluded)\")\n",
        "\n",
        "ax.set_xlabel(\"Global optimizer step\")\n",
        "ax.set_ylabel(\"Tokens / second\")\n",
        "ax.set_title(\"Training throughput — tokens per second\")\n",
        "ax.grid(True, linestyle=\"--\", linewidth=0.4, alpha=0.7)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "fpath = os.path.join(OUT_DIR, \"tokens_per_sec_over_steps.png\")\n",
        "plt.savefig(fpath, bbox_inches=\"tight\")\n",
        "print(f\"Saved figure: {fpath}\")\n",
        "plt.show()\n",
        "\n",
        "# ---- Figure 2: Samples/sec & per-epoch aggregation ----\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x, df[\"samples_per_sec\"], alpha=0.25, label=\"samples/sec (per step)\")\n",
        "ax.plot(x, df[\"samples_per_sec_roll\"], label=f\"samples/sec (rolling, window={ROLL_WINDOW})\")\n",
        "\n",
        "# If epoch column exists, draw vertical boundaries and annotate epoch averages\n",
        "if \"epoch\" in df.columns:\n",
        "    epochs = df[\"epoch\"].fillna(method=\"ffill\").unique()\n",
        "    for e in epochs:\n",
        "        # find first index of epoch e\n",
        "        idxs = df.index[df[\"epoch\"]==e].tolist()\n",
        "        if not idxs:\n",
        "            continue\n",
        "        ax.axvline(x=idxs[0], linestyle=\":\", linewidth=0.7)\n",
        "        # epoch mean\n",
        "        e_mean = df.loc[df[\"epoch\"]==e, \"samples_per_sec\"].mean()\n",
        "        ax.text(idxs[0]+0.5, ax.get_ylim()[1]*0.95, f\"Epoch {int(e)} mean: {e_mean:.1f}\", fontsize=9, verticalalignment='top')\n",
        "\n",
        "ax.set_xlabel(\"Global optimizer step\")\n",
        "ax.set_ylabel(\"Samples / second (examples/sec)\")\n",
        "ax.set_title(\"Training throughput — samples per second\")\n",
        "ax.grid(True, linestyle=\"--\", linewidth=0.4, alpha=0.7)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "fpath = os.path.join(OUT_DIR, \"samples_per_sec_over_steps.png\")\n",
        "plt.savefig(fpath, bbox_inches=\"tight\")\n",
        "print(f\"Saved figure: {fpath}\")\n",
        "plt.show()\n",
        "\n",
        "# ---- Optional: Small table of summary stats saved to CSV for the thesis ----\n",
        "pd.Series(summary).to_csv(os.path.join(OUT_DIR, \"throughput_summary_stats.csv\"))\n",
        "print(\"Summary statistics saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ocp-CufKrXME"
      },
      "source": [
        "##Visualizing LoRA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkuHqgeYAB1Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data from the mbpp_adapter\n",
        "data = {\n",
        "    \"Epoch\": [1, 2, 3, 4, 5],\n",
        "    \"Training Loss\": [2.341400, 2.157700, 1.956200, 1.832300, 1.803000],\n",
        "    \"Validation Loss\": [2.353944, 2.135633, 2.006576, 1.915481, 1.880655],\n",
        "    \"Bleu\": [0.193154, 0.194993, 0.202520, 0.201009, 0.200844],\n",
        "    \"RougeL\": [0.533877, 0.536636, 0.555202, 0.553825, 0.553992],\n",
        "    \"Exact Match\": [0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "    \"Avg Edit Distance\": [1043.011111, 1047.111111, 1044.511111, 1045.600000, 1046.400000],\n",
        "    \"Entropy\": [4.754701, 4.759662, 4.767116, 4.765970, 4.765502],\n",
        "    \"Num Tokens\": [191488, 382976, 574464, 765952, 957440],\n",
        "    \"Mean Token Accuracy\": [0.639754, 0.650686, 0.666477, 0.667128, 0.667628],\n",
        "}\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display nicely\n",
        "pd.set_option(\"display.precision\", 6)\n",
        "print(df)\n",
        "\n",
        "# Save to CSV for use in plotting / thesis figures\n",
        "df.to_csv(\"training_metrics_table.csv\", index=False)\n",
        "print(\"\\nSaved CSV -> training_metrics_table.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx4miqrQpgOw"
      },
      "source": [
        "### Evaluation Metrics – BLEU, ROUGE-L, Exact Match\n",
        "hows model generation quality improving (or stabilizing) across epochs. These are standard text generation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxnjJZHurbL5"
      },
      "outputs": [],
      "source": [
        "# File: analysis_plots For GPT-2 LoRA on MBPP and Codealpaca Adapters.py\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "sns.set(style=\"whitegrid\", context=\"paper\", font_scale=1.1)\n",
        "\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jep4GvjupYaC"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.lineplot(data=df, x=\"Epoch\", y=\"BLEU\", marker=\"o\", label=\"BLEU\")\n",
        "sns.lineplot(data=df, x=\"Epoch\", y=\"ROUGE-L\", marker=\"o\", label=\"ROUGE-L\")\n",
        "sns.lineplot(data=df, x=\"Epoch\", y=\"Exact Match\", marker=\"o\", label=\"Exact Match\")\n",
        "\n",
        "plt.title(\"Evaluation Metrics Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSOu3qo3uM06"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "OUTDIR = \"figures\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# === Data from your table ===\n",
        "Data = pd.DataFrame({\n",
        "    \"Epoch\":           [1, 2, 3, 4, 5],\n",
        "    \"Training Loss\":   [2.341400, 2.157700, 1.956200, 1.832300, 1.803000],\n",
        "    \"Validation Loss\": [2.353944, 2.135633, 2.006576, 1.915148, 1.880655],\n",
        "    \"Bleu\":            [0.193154, 0.194993, 0.202520, 0.201009, 0.200844],\n",
        "    \"ROUGE-L\":         [0.533877, 0.536636, 0.555202, 0.553825, 0.553992],\n",
        "    \"Exact Match\":     [0.0, 0.0, 0.0, 0.0, 0.0],       # constant zero - not informative\n",
        "    \"Avg Edit Distance\":[1043.011111, 1047.111111, 1044.511111, 1045.600000, 1046.400000],\n",
        "    \"Entropy\":         [4.754701, 4.759662, 4.767116, 4.765970, 4.765502],\n",
        "    \"Num Tokens\":      [191488.0, 382976.0, 574464.0, 765952.0, 957440.0],\n",
        "    \"Mean Token Accuracy\":[0.639754, 0.650686, 0.666477, 0.667128, 0.667628],\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFara13SuHg3"
      },
      "outputs": [],
      "source": [
        "# Training & Validation Loss vs Epoch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "sns.set(context=\"paper\", style=\"whitegrid\", font_scale=1.1)\n",
        "\n",
        "# Use previously defined Data\n",
        "# Columns used: Epoch, Training Loss, Validation Loss\n",
        "plt.figure(figsize=(9,5))\n",
        "ax = sns.lineplot(data=Data, x=\"Epoch\", y=\"Training Loss\", marker=\"o\", label=\"Training Loss\")\n",
        "sns.lineplot(data=Data, x=\"Epoch\", y=\"Validation Loss\", marker=\"s\", label=\"Validation Loss\", ax=ax)\n",
        "\n",
        "# Annotate values above validation loss points\n",
        "for x, y in zip(Data[\"Epoch\"], Data[\"Validation Loss\"]):\n",
        "    ax.annotate(f\"{y:.3f}\", (x, y), textcoords=\"offset points\", xytext=(0,6), ha='center', fontsize=9)\n",
        "\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Loss\")\n",
        "ax.set_xticks(Data[\"Epoch\"])\n",
        "ax.set_title(\"Training and Validation Loss vs Epoch (GPT-2 LoRA)\")\n",
        "ax.grid(True, linestyle=\"--\", linewidth=0.45, alpha=0.9)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"loss_vs_epoch.png\", dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrT8XL__smmE"
      },
      "outputs": [],
      "source": [
        "# Evaluation Metrics (BLEU, ROUGE-L, Exact Match)\n",
        "import matplotlib.ticker as ticker\n",
        "plt.figure(figsize=(9,5))\n",
        "\n",
        "# Columns to plot: BLEU, ROUGE-L, Exact Match\n",
        "metrics_to_plot = [\"Bleu\", \"ROUGE-L\", \"Exact Match\"]\n",
        "\n",
        "# Plot each metric\n",
        "for metric in metrics_to_plot:\n",
        "    plt.plot(Data[\"Epoch\"], Data[metric]*100 if metric!=\"Exact Match\" else Data[metric],\n",
        "             marker=\"o\", label=metric)\n",
        "\n",
        "# Annotate last point of each metric\n",
        "for metric in metrics_to_plot:\n",
        "    x_last = Data[\"Epoch\"].iloc[-1]\n",
        "    y_last = Data[metric].iloc[-1]*100 if metric!=\"Exact Match\" else Data[metric].iloc[-1]\n",
        "    plt.annotate(f\"{y_last:.3f}\", (x_last, y_last), textcoords=\"offset points\", xytext=(6,0), ha='left', fontsize=9)\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score (%)\")\n",
        "plt.title(\"Evaluation Metrics over Epochs (BLEU, ROUGE-L, Exact Match)\")\n",
        "plt.xticks(Data[\"Epoch\"])\n",
        "plt.grid(True, linestyle=\"--\", linewidth=0.45, alpha=0.9)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"eval_metrics_over_epochs.png\", dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcjJ9tgEvBT9"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDQqhlF3q41r"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# Plot : Mean Token Accuracy (left) & Entropy (right) vs epoch (twin axis)\n",
        "# ===================================================================\n",
        "plt.figure(figsize=(8,5))\n",
        "ax1 = plt.gca()\n",
        "ax2 = ax1.twinx()\n",
        "sns.lineplot(x=\"Epoch\", y=\"Mean Token Accuracy\", data=df, marker=\"o\", color=\"tab:blue\", label=\"Mean Token Accuracy\", ax=ax1)\n",
        "sns.lineplot(x=\"Epoch\", y=\"Entropy\", data=df, marker=\"s\", color=\"tab:orange\", label=\"Entropy\", ax=ax2)\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Mean Token Accuracy\")\n",
        "ax2.set_ylabel(\"Entropy (per-token)\")\n",
        "ax1.set_xticks(df[\"Epoch\"])\n",
        "ax1.set_ylim(0.60, 0.70)\n",
        "ax2.set_ylim(df['Entropy'].min()*0.995, df['Entropy'].max()*1.005)\n",
        "ax1.set_title(\"Token Accuracy vs Entropy\")\n",
        "ax1.legend(loc=\"upper left\")\n",
        "ax2.legend([plt.Line2D([0],[0], color='tab:orange')], [\"Entropy\"], loc=\"upper right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"token_acc_entropy.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVB5EW-eP18P"
      },
      "source": [
        "##Visualizing LoRA Model with MBPP Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA99ApRqZa3O"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "epochs = [1, 2, 3, 4, 5]\n",
        "train_loss = [3.0928, 2.9043, 2.5556, 2.4169, 2.4171]\n",
        "val_loss   = [2.8876, 2.6947, 2.5502, 2.4668, 2.4399]\n",
        "bleu       = [0.1255, 0.1192, 0.1156, 0.1129, 0.1115]\n",
        "rougeL     = [0.4431, 0.4469, 0.4680, 0.4768, 0.4793]\n",
        "accuracy   = [0.5760, 0.5791, 0.5871, 0.5900, 0.5914]\n",
        "entropy    = [4.2323, 4.3061, 4.3552, 4.3781, 4.3863]\n",
        "\n",
        "# === 1. Training and Validation Loss ===\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, train_loss, marker='o', label='Train Loss')\n",
        "plt.plot(epochs, val_loss, marker='o', label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss Over Epochs\")\n",
        "plt.legend(); plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"loss_over_epochs.pdf\", dpi=300)   # save as PDF\n",
        "plt.savefig(\"loss_over_epochs.png\", dpi=300)   # optional PNG\n",
        "#plt.close()\n",
        "\n",
        "# === 2. BLEU Score ===\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, bleu, marker='o', color='C2')\n",
        "plt.xlabel(\"Epochs\"); plt.ylabel(\"BLEU Score\")\n",
        "plt.title(\"BLEU Score Over Epochs\")\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"bleu_over_epochs.pdf\", dpi=300)\n",
        "plt.savefig(\"bleu_over_epochs.png\", dpi=300)\n",
        "#plt.close()\n",
        "\n",
        "# === 3. ROUGE-L Score ===\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, rougeL, marker='o', color='C3')\n",
        "plt.xlabel(\"Epochs\"); plt.ylabel(\"ROUGE-L Score\")\n",
        "plt.title(\"ROUGE-L Score Over Epochs\")\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"rougeL_over_epochs.pdf\", dpi=300)\n",
        "plt.savefig(\"rougeL_over_epochs.png\", dpi=300)\n",
        "#plt.close()\n",
        "\n",
        "# === 4. Token Accuracy ===\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, marker='o', color='C4')\n",
        "plt.xlabel(\"Epochs\"); plt.ylabel(\"Mean Token Accuracy\")\n",
        "plt.title(\"Token Prediction Accuracy Over Epochs\")\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"accuracy_over_epochs.pdf\", dpi=300)\n",
        "plt.savefig(\"accuracy_over_epochs.png\", dpi=300)\n",
        "#plt.close()\n",
        "\n",
        "# === 5. Entropy ===\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, entropy, marker='o', color='C5')\n",
        "plt.xlabel(\"Epochs\"); plt.ylabel(\"Entropy\")\n",
        "plt.title(\"Entropy of Predictions Over Epochs\")\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"entropy_over_epochs.pdf\", dpi=300)\n",
        "plt.savefig(\"entropy_over_epochs.png\", dpi=300)\n",
        "#plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1qNzks5ZzwO"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"loss_over_epochs.pdf\")\n",
        "files.download(\"rougeL_over_epochs.pdf\")\n",
        "files.download(\"bleu_over_epochs.pdf\")\n",
        "files.download(\"accuracy_over_epochs.pdf\")\n",
        "files.download(\"entropy_over_epochs.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUku2Q-IQizm"
      },
      "source": [
        "###Training and Validation Loss Over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdkpuy3UQkZu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training and validation loss curves\n",
        "epochs = [1, 2, 3, 4, 5]\n",
        "train_loss = [3.0928, 2.9043, 2.5556, 2.4169, 2.4171]\n",
        "val_loss   = [2.8876, 2.6947, 2.5502, 2.4668, 2.4399]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, train_loss, marker='o', color='C0', label='Train Loss')\n",
        "plt.plot(epochs, val_loss,   marker='o', color='C1', label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5PDkAUoQnQL"
      },
      "source": [
        "###BLEU Score Over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEbMyoTGQm2b"
      },
      "outputs": [],
      "source": [
        "# Plot BLEU score curve\n",
        "bleu = [0.1255, 0.1192, 0.1156, 0.1129, 0.1115]  # from epochs 1–5\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, bleu, marker='o', color='C2')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('BLEU Score')\n",
        "plt.ylim(0.10, 0.13)\n",
        "plt.title('BLEU Score During Training')\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsPgKgDaQvoR"
      },
      "source": [
        "###ROUGE-L Score Over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMb8ieFeQzhn"
      },
      "outputs": [],
      "source": [
        "# Plot ROUGE-L score curve\n",
        "rouge = [0.4431, 0.4469, 0.4680, 0.4768, 0.4793]  # from epochs 1–5\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, rouge, marker='o', color='C3')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('ROUGE-L Score')\n",
        "plt.ylim(0.44, 0.50)\n",
        "plt.title('ROUGE-L Score During Training')\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YWVsgsemDiN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "epochs = [1, 2, 3, 4, 5]\n",
        "bleu = [0.1255, 0.1192, 0.1156, 0.1129, 0.1115]  # from epochs 1–5\n",
        "rouge = [0.4431, 0.4469, 0.4680, 0.4768, 0.4793]  # from epochs 1–5\n",
        "\n",
        "plt.figure(figsize=(8, 5)) # Increased figure size for better readability\n",
        "\n",
        "# Plot BLEU score\n",
        "plt.plot(epochs, bleu, marker='o', color='C2', label='BLEU Score')\n",
        "\n",
        "# Plot ROUGE-L score\n",
        "plt.plot(epochs, rouge, marker='o', color='C3', label='ROUGE-L Score')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "# Set a title that reflects both metrics\n",
        "plt.title('BLEU and ROUGE-L Scores Over Epochs')\n",
        "plt.legend() # Add a legend to distinguish the lines\n",
        "plt.grid(True, linestyle='--', linewidth=0.5) # Add a grid\n",
        "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlOnMspTQ3mN"
      },
      "source": [
        "###Mean Token Accuracy Over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVH48ih9Q6_2"
      },
      "outputs": [],
      "source": [
        "# Plot mean token accuracy curve\n",
        "acc = [0.5760, 0.5791, 0.5871, 0.5900, 0.5914]  # from epochs 1–5\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, acc, marker='o', color='C4')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Token Accuracy')\n",
        "plt.ylim(0.57, 0.60)\n",
        "plt.title('Token Prediction Accuracy During Training')\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKX85K1xRA50"
      },
      "source": [
        "###Entropy of Model Predictions Over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtiySTTARCvI"
      },
      "outputs": [],
      "source": [
        "# Plot prediction entropy curve\n",
        "entropy = [4.2323, 4.3061, 4.3552, 4.3781, 4.3863]  # from epochs 1–5\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, entropy, marker='o', color='C5')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Entropy')\n",
        "plt.title('Entropy of Predictions Over Epochs')\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKLimTj5RZ1-"
      },
      "source": [
        "### Trainable Parameter Count: Full Model vs LoRA Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fliZsYxJRbgq"
      },
      "outputs": [],
      "source": [
        "# Bar chart of trainable parameters (example values)\n",
        "import numpy as np\n",
        "\n",
        "params = {'Full Model': 124.0,    # e.g., 124M total parameters\n",
        "          'LoRA (r=4) Modules': 0.15}  # e.g., ~0.15M from LoRA layers\n",
        "labels = list(params.keys())\n",
        "vals   = list(params.values())\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "bars = plt.bar(labels, vals, color=['#ff8080','#80b3ff'])\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Trainable Parameters (millions, log scale)')\n",
        "plt.title('Trainable Parameters: Full Model vs LoRA (r=4)')\n",
        "# Annotate absolute values\n",
        "plt.text(0, vals[0]*1.1, f\"{vals[0]:.1f}M\", ha='center')\n",
        "plt.text(1, vals[1]*8,    f\"{vals[1]:.2f}M\", ha='center')\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsukJKKQmHEK"
      },
      "source": [
        "##Inference for NLG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tjMPss2CkVOi"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizergpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# Load the tokenizer and model\n",
        "modelgpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR_RUsoLZSqM"
      },
      "outputs": [],
      "source": [
        "base_tokenizer=tokenizergpt2\n",
        "base_model= modelgpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ7fYlD_cdb0"
      },
      "outputs": [],
      "source": [
        "model= lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AteOUw2lJHQ"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "#prompt = \"Once upon a time\"\n",
        "prompt= \"Write a Python function to check if a number is prime.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF5_DrRDly4i"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "output_ids = model.generate(\n",
        "    inputs.input_ids.to(model.device),\n",
        "    attention_mask=inputs.attention_mask.to(model.device),\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    max_length=50,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "output_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTs7Zp4xl2Bj"
      },
      "outputs": [],
      "source": [
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5pu1VHElR6x9",
        "lFut5KSW8Vo-",
        "PXesld71cQUF",
        "oRbez1ZUZiOO"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}